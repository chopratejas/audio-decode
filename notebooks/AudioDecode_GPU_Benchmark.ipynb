{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AudioDecode GPU Benchmark\n",
    "\n",
    "This notebook runs comprehensive GPU benchmarks for AudioDecode vs OpenAI Whisper.\n",
    "\n",
    "**Before running:**\n",
    "1. Runtime â†’ Change runtime type â†’ GPU (T4, A100, or V100)\n",
    "2. Run all cells in order\n",
    "\n",
    "**Expected results:**\n",
    "- T4 GPU: ~3-4x faster than OpenAI Whisper\n",
    "- A100 GPU: ~4-5x faster than OpenAI Whisper\n",
    "\n",
    "**Time estimate:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  NO GPU DETECTED!\")\n",
    "    print(\"Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "    raise RuntimeError(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq ffmpeg libsndfile1 > /dev/null 2>&1\n",
    "print(\"âœ“ System dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone AudioDecode repository\n",
    "import os\n",
    "if not os.path.exists('audiodecode'):\n",
    "    !git clone https://github.com/YOUR_USERNAME/audiodecode.git\n",
    "    print(\"âœ“ Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ Repository already exists\")\n",
    "\n",
    "%cd audiodecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_audiodecode"
   },
   "outputs": [],
   "source": [
    "# Install AudioDecode and dependencies\n",
    "!pip install -q -e \".[dev,inference]\"\n",
    "!pip install -q faster-whisper openai-whisper yt-dlp\n",
    "print(\"âœ“ AudioDecode installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_install"
   },
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import audiodecode\n",
    "import faster_whisper\n",
    "import whisper\n",
    "print(f\"âœ“ AudioDecode version: {audiodecode.__version__}\")\n",
    "print(f\"âœ“ faster-whisper installed\")\n",
    "print(f\"âœ“ openai-whisper installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_benchmark"
   },
   "outputs": [],
   "source": [
    "# Run GPU benchmark\n",
    "print(\"ðŸš€ Starting GPU benchmark...\\n\")\n",
    "print(\"This will:\")\n",
    "print(\"1. Download test audio from YouTube (~6.7 min)\")\n",
    "print(\"2. Benchmark OpenAI Whisper (GPU)\")\n",
    "print(\"3. Benchmark AudioDecode (GPU)\")\n",
    "print(\"4. Compare results\\n\")\n",
    "print(\"Expected time: 5-10 minutes\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "!python benchmark_vs_openai_whisper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_results"
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nðŸ“Š Full results saved to: BENCHMARK_VS_OPENAI_WHISPER.md\\n\")\n",
    "with open('BENCHMARK_VS_OPENAI_WHISPER.md', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Download results file\n",
    "from google.colab import files\n",
    "print(\"ðŸ“¥ Downloading results file...\")\n",
    "files.download('BENCHMARK_VS_OPENAI_WHISPER.md')\n",
    "print(\"âœ“ Download complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Update README.md with GPU numbers\")\n",
    "print(\"2. Update CRITICAL_GAPS_ANALYSIS.md (mark GPU gap as FIXED)\")\n",
    "print(\"3. Update PLATFORM_BENCHMARK_COMPARISON.md with GPU data\")\n",
    "print(\"4. Contact design partners with complete benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes\n",
    "\n",
    "**GPU Performance:**\n",
    "- T4 GPU: ~3-4x faster than OpenAI Whisper (estimated)\n",
    "- A100 GPU: ~4-5x faster than OpenAI Whisper (estimated)\n",
    "\n",
    "**Cost:**\n",
    "- Google Colab: Free (T4) or $10/month (Colab Pro)\n",
    "- RunPod RTX 4090: $0.34/hour (~$0.24 for complete benchmark)\n",
    "- Lambda Labs A100: $1.10/hour\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If out of memory: Restart runtime and try with smaller model (tiny or base)\n",
    "- If slow download: Models download from HuggingFace (first run only)\n",
    "- If CUDA errors: Make sure GPU runtime is selected"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
